{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9489760,"sourceType":"datasetVersion","datasetId":5773582},{"sourceId":9490030,"sourceType":"datasetVersion","datasetId":5773762},{"sourceId":9586995,"sourceType":"datasetVersion","datasetId":5846609},{"sourceId":9610079,"sourceType":"datasetVersion","datasetId":5863841},{"sourceId":9614670,"sourceType":"datasetVersion","datasetId":5867262}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom torch.optim.lr_scheduler import StepLR\nfrom sklearn.metrics import f1_score\nfrom tqdm import tqdm\nimport pandas as pd\nfrom types import SimpleNamespace\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport random\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:22:32.240170Z","iopub.execute_input":"2024-10-14T06:22:32.240601Z","iopub.status.idle":"2024-10-14T06:22:32.247785Z","shell.execute_reply.started":"2024-10-14T06:22:32.240562Z","shell.execute_reply":"2024-10-14T06:22:32.246794Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter","metadata":{}},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"#augmented, 500개로 맞춤\n#augmented2, 1000개로 맞춤\n#augmented_combined, 동일 카테고리별로 5개씩 묶고, 합친 갯수가 200개보다 적으면 200개까지 증강\n#augmented_combined3, 동일 카테고리별로 2개씩 묶고, 합친 갯수가 200개보다 적으면 50개까지 증강\n#augmented_combined4, 셔플 추가, 동일 카테고리별로 2개씩 묶고, 합친 갯수가 200개보다 적으면 50개까지 증강\n#augmented_combined5, 셔플 추가, 동일 카테고리별로 2개씩 묶고, 합친 갯수가 200개보다 적으면 50개까지 증강\n#그리고 지역은 5000개까지 줄임\n#augmented_combined6, 셔플 추가, 동일 카테고리별로 2개씩 묶고, 합친 갯수가 30개보다 적으면 30개까지 증강\n#augmented_combined7, 셔플 추가, 동일 카테고리별로 2개씩 묶고, 합친 갯수가 30개보다 적으면 30개까지 증강\n#그리고 지역은 7000개까지 줄임\n#augmented_combined8, 셔플 추가, 동일 카테고리별로 2개씩 묶고, 합친 갯수가 30개보다 적으면 30개까지 증강\n#그리고 지역은 5000개까지 줄임\n#augmented_combined7, 셔플 추가, 동일 카테고리별로 2개씩 묶고, 합친 갯수가 30개보다 적으면 30개까지 증강\n#그리고 지역은 3000개까지 줄임\n#augmented_combined7, 셔플 추가, 동일 카테고리별로 2개씩 묶고, 합친 갯수가 30개보다 적으면 30개까지 증강\n#그리고 지역은 2000개까지 줄임\ntrain_df = pd.read_csv(\"/kaggle/input/dacon-dataset/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/dacon-dataset/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:27.090476Z","iopub.execute_input":"2024-10-14T06:16:27.090812Z","iopub.status.idle":"2024-10-14T06:16:29.151372Z","shell.execute_reply.started":"2024-10-14T06:16:27.090778Z","shell.execute_reply":"2024-10-14T06:16:29.150332Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train_df[\"분류\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:29.154304Z","iopub.execute_input":"2024-10-14T06:16:29.154707Z","iopub.status.idle":"2024-10-14T06:16:29.170066Z","shell.execute_reply.started":"2024-10-14T06:16:29.154663Z","shell.execute_reply":"2024-10-14T06:16:29.168995Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"분류\n지역               26950\n경제:부동산            3454\n사회:사건_사고          2568\n경제:반도체            2318\n사회:사회일반           1480\n사회:교육_시험           995\n정치:국회_정당           966\n사회:의료_건강           950\n경제:취업_창업           845\n스포츠:올림픽_아시안게임      841\n경제:산업_기업           711\n문화:전시_공연           671\n경제:자동차             640\n경제:경제일반            625\n사회:장애인             621\n스포츠:골프             617\n정치:선거              608\n경제:유통              589\nIT_과학:모바일          537\n사회:여성              536\n사회:노동_복지           447\n사회:환경              396\n경제:서비스_쇼핑          387\n경제:무역              375\n정치:행정_자치           349\n국제                 337\n문화:방송_연예           335\n스포츠:축구             328\n경제:금융_재테크          327\n정치:청와대             279\n문화:출판              248\nIT_과학:IT_과학일반      243\nIT_과학:인터넷_SNS      238\n문화:미술_건축           229\n정치:정치일반            221\nIT_과학:과학           215\n문화:문화일반            213\n문화:학술_문화재          202\n문화:요리_여행           190\n경제:자원              178\n문화:종교              173\nIT_과학:콘텐츠          160\n사회:미디어             128\n사회:날씨              124\n스포츠:농구_배구          113\n문화:음악              110\n문화:생활              102\nIT_과학:보안            94\n스포츠:월드컵             90\n경제:증권_증시            76\n정치:북한               67\n정치:외교               31\n스포츠:스포츠일반           29\n문화:영화               27\n스포츠:야구              17\n경제:외환                9\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"#중복만 날림","metadata":{}},{"cell_type":"code","source":"# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.metrics.pairwise import cosine_similarity\n# import random\n\n# # 1. '지역' 카테고리 필터링\n# region_df = train_df[train_df['분류'] == '지역'].copy()\n\n# # 2. 제목과 키워드 데이터를 각각 리스트로 변환\n# title_texts = region_df['제목'].tolist()\n# keyword_texts = region_df['키워드'].tolist()\n\n# # 3. TF-IDF 벡터화\n# title_vectorizer = TfidfVectorizer().fit_transform(title_texts)\n# keyword_vectorizer = TfidfVectorizer().fit_transform(keyword_texts)\n\n# # 4. 코사인 유사도 계산 (제목과 키워드 각각)\n# title_cosine_similarities = cosine_similarity(title_vectorizer, title_vectorizer)\n# keyword_cosine_similarities = cosine_similarity(keyword_vectorizer, keyword_vectorizer)\n\n# # 5. 유사도 임계값 설정 (0.8 이상이면 중복으로 간주)\n# threshold = 0.5\n\n# # 6. 중복 데이터 탐지 함수 (일부 제거)\n# def find_duplicate_indices(cosine_similarities, threshold):\n#     duplicate_indices = []\n#     for i in range(len(cosine_similarities)):\n#         for j in range(i + 1, len(cosine_similarities)):\n#             if cosine_similarities[i][j] > threshold:\n#                 duplicate_indices.append(j)\n#     # 중복된 데이터 중에서 절반만 제거하도록 선택\n#     unique_duplicate_indices = list(set(duplicate_indices))\n#     to_remove = random.sample(unique_duplicate_indices, len(unique_duplicate_indices) // 2)\n#     return to_remove\n\n# # 7. 제목과 키워드에서 각각 유사한 데이터의 중복 인덱스 찾기\n# title_duplicate_indices = find_duplicate_indices(title_cosine_similarities, threshold)\n# keyword_duplicate_indices = find_duplicate_indices(keyword_cosine_similarities, threshold)\n\n# # 8. 제목과 키워드 중복 인덱스의 교집합만 제거\n# duplicate_indices = list(set(title_duplicate_indices) | set(keyword_duplicate_indices)) \n# # 9. 중복 데이터 제거\n# region_df_cleaned = region_df.drop(region_df.index[duplicate_indices]).reset_index(drop=True)\n\n\n# # 10. 중복된 데이터 수 확인\n# print(f\"지역 카테고리에서 중복된 데이터 {len(duplicate_indices)}개 제거 완료.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:29.171570Z","iopub.execute_input":"2024-10-14T06:16:29.172351Z","iopub.status.idle":"2024-10-14T06:16:29.180412Z","shell.execute_reply.started":"2024-10-14T06:16:29.172313Z","shell.execute_reply":"2024-10-14T06:16:29.179429Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# 11. 원래 데이터프레임에서 지역 카테고리를 업데이트\n# train_df_updated = pd.concat([train_df[train_df['분류'] != '지역'], region_df_cleaned], ignore_index=True)\n\n# 제거된 후의 데이터프레임 확인\n#print(f\"전체 데이터 크기: {train_df_updated.shape}\")\n# print(f\"전체 데이터 크기: {train_df.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:29.184475Z","iopub.execute_input":"2024-10-14T06:16:29.184896Z","iopub.status.idle":"2024-10-14T06:16:29.195325Z","shell.execute_reply.started":"2024-10-14T06:16:29.184770Z","shell.execute_reply":"2024-10-14T06:16:29.194247Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"#train_df_updated[\"분류\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:29.196504Z","iopub.execute_input":"2024-10-14T06:16:29.196846Z","iopub.status.idle":"2024-10-14T06:16:29.207728Z","shell.execute_reply.started":"2024-10-14T06:16:29.196812Z","shell.execute_reply":"2024-10-14T06:16:29.206785Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"#비슷한거 날리고 2개 합침（새로운 형태의 데이터가 필요하면 변경）\n","metadata":{}},{"cell_type":"code","source":"def combine_articles(df):\n    combined_data = []\n    removed_count = 0  # 삭제된 데이터 수를 추적\n    threshold = 0.7  # 유사도 임계값\n\n    for category in df['분류'].unique():\n        category_data = df[df['분류'] == category].reset_index(drop=True)\n\n        # 카테고리 데이터가 10,000개 이상일 경우에만 진행, 지역만 적용됨\n        if len(category_data) > 10000:\n\n            # 제목과 키워드를 합친 텍스트 데이터 생성\n            category_data['combined_text'] = category_data['제목'] + \" \" + category_data['키워드']\n\n            # TF-IDF 벡터화\n            vectorizer = TfidfVectorizer()\n            tfidf_matrix = vectorizer.fit_transform(category_data['combined_text'])\n\n            # 코사인 유사도 계산\n            cosine_sim = cosine_similarity(tfidf_matrix)\n\n            # 유사한 기사 2개씩 묶기 + 일부 중복 제거\n            used_indices = set()\n\n            # 중복 제거를 위해 일정 임계값 이상의 유사도를 가진 데이터 탐색\n            duplicate_indices = []\n            for i in range(len(cosine_sim)):\n                for j in range(i + 1, len(cosine_sim)):\n                    if cosine_sim[i][j] > threshold:\n                        duplicate_indices.append(j)\n\n            # 중복된 데이터의 절반만 제거\n            unique_duplicate_indices = list(set(duplicate_indices))\n            to_remove = random.sample(unique_duplicate_indices, len(unique_duplicate_indices) // 2)\n            removed_count += len(to_remove)  # 삭제된 데이터 수 기록\n\n            # 중복 제거 후, 인덱스가 올바른지 확인\n            for i in range(len(category_data)):\n                if i in used_indices or i in to_remove:\n                    continue\n                \n                # 가장 유사한 기사 선택\n                most_similar_idx = np.argsort(cosine_sim[i])[-2]  # 자기 자신을 제외한 가장 유사한 기사 선택\n                \n                # most_similar_idx가 사용되었거나 제거된 항목인지 확인\n                if most_similar_idx in used_indices or most_similar_idx in to_remove:\n                    continue\n                \n                combined_title = category_data['제목'][i] + \" \" + category_data['제목'][most_similar_idx]\n                combined_keywords = category_data['키워드'][i] + \",\" + category_data['키워드'][most_similar_idx]\n                \n                combined_entry = {\n                    '분류': category,\n                    '제목': combined_title,\n                    '키워드': combined_keywords\n                }\n                \n                combined_data.append(combined_entry)\n                used_indices.add(i)\n                used_indices.add(most_similar_idx)\n\n        else:\n            # 데이터가 10,000개 이하인 경우 2개씩 단순 묶기\n            for i in range(0, len(category_data), 1):\n                combined_title = \" \".join(category_data['제목'][i:i+2])\n                combined_keywords = \",\".join(category_data['키워드'][i:i+2])\n                combined_entry = {\n                    '분류': category,\n                    '제목': combined_title,\n                    '키워드': combined_keywords\n                }\n                combined_data.append(combined_entry)\n\n    # 총 몇 개의 데이터가 합병되었는지 확인 출력\n    print(f\"총 {removed_count}개의 데이터가 유사해서 합병 또는 제거되었습니다.\")\n    \n    combined_df = pd.DataFrame(combined_data)\n\n    # CSV로 저장\n    combined_df.to_csv(\"/kaggle/working/editted.csv\", index=False, encoding='utf-8-sig')\n    print(\"CSV 파일로 저장되었습니다: editted.csv\")\n    \n    return combined_df\n\n# 사용 예시\n# df = pd.DataFrame({'분류': [...], '제목': [...], '키워드': [...]})\n# new_df = combine_articles(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:29.209238Z","iopub.execute_input":"2024-10-14T06:16:29.209578Z","iopub.status.idle":"2024-10-14T06:16:29.225303Z","shell.execute_reply.started":"2024-10-14T06:16:29.209541Z","shell.execute_reply":"2024-10-14T06:16:29.224347Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"combined_df = combine_articles(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:29.226315Z","iopub.execute_input":"2024-10-14T06:16:29.226595Z","iopub.status.idle":"2024-10-14T06:20:41.412007Z","shell.execute_reply.started":"2024-10-14T06:16:29.226565Z","shell.execute_reply":"2024-10-14T06:20:41.411016Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"총 6492개의 데이터가 유사해서 합병 또는 제거되었습니다.\nCSV 파일로 저장되었습니다: editted.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"final_df=combined_df","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:20:41.413081Z","iopub.execute_input":"2024-10-14T06:20:41.413384Z","iopub.status.idle":"2024-10-14T06:20:41.426425Z","shell.execute_reply.started":"2024-10-14T06:20:41.413353Z","shell.execute_reply":"2024-10-14T06:20:41.425448Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# 줄인 데이터 확인\nprint(final_df['분류'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:20:41.428605Z","iopub.execute_input":"2024-10-14T06:20:41.428970Z","iopub.status.idle":"2024-10-14T06:20:41.442559Z","shell.execute_reply.started":"2024-10-14T06:20:41.428925Z","shell.execute_reply":"2024-10-14T06:20:41.441538Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"분류\n지역               6289\n경제:부동산           3454\n사회:사건_사고         2568\n경제:반도체           2318\n사회:사회일반          1480\n사회:교육_시험          995\n정치:국회_정당          966\n사회:의료_건강          950\n경제:취업_창업          845\n스포츠:올림픽_아시안게임     841\n경제:산업_기업          711\n문화:전시_공연          671\n경제:자동차            640\n경제:경제일반           625\n사회:장애인            621\n스포츠:골프            617\n정치:선거             608\n경제:유통             589\nIT_과학:모바일         537\n사회:여성             536\n사회:노동_복지          447\n사회:환경             396\n경제:서비스_쇼핑         387\n경제:무역             375\n정치:행정_자치          349\n국제                337\n문화:방송_연예          335\n스포츠:축구            328\n경제:금융_재테크         327\n정치:청와대            279\n문화:출판             248\nIT_과학:IT_과학일반     243\nIT_과학:인터넷_SNS     238\n문화:미술_건축          229\n정치:정치일반           221\nIT_과학:과학          215\n문화:문화일반           213\n문화:학술_문화재         202\n문화:요리_여행          190\n경제:자원             178\n문화:종교             173\nIT_과학:콘텐츠         160\n사회:미디어            128\n사회:날씨             124\n스포츠:농구_배구         113\n문화:음악             110\n문화:생활             102\nIT_과학:보안           94\n스포츠:월드컵            90\n경제:증권_증시           76\n정치:북한              67\n정치:외교              31\n스포츠:스포츠일반          29\n문화:영화              27\n스포츠:야구             17\n경제:외환               9\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Custom Dataset","metadata":{}},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        label = self.labels[item] if self.labels is not None else -1\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'text': text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:20:41.445996Z","iopub.execute_input":"2024-10-14T06:20:41.446298Z","iopub.status.idle":"2024-10-14T06:20:41.454400Z","shell.execute_reply.started":"2024-10-14T06:20:41.446266Z","shell.execute_reply":"2024-10-14T06:20:41.453495Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# # 데이터 준비\n# train_df['제목_키워드'] = train_df['제목'] + ' ' + train_df['키워드']\n# test_df['제목_키워드'] = test_df['제목'] + ' ' + test_df['키워드']\n\n# # 레이블 인코딩\n# label_encoder = {label: i for i, label in enumerate(train_df['분류'].unique())}\n# train_df['label'] = train_df['분류'].map(label_encoder)\n\n# # 데이터 분할 (train -> train + validation)\n# train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['분류'], random_state=42)\n\n# # 데이터셋 생성\n# train_dataset = TextDataset(train_df.제목_키워드.tolist(), train_df.label.tolist(), tokenizer)\n# val_dataset = TextDataset(val_df.제목_키워드.tolist(), val_df.label.tolist(), tokenizer)\n# test_dataset = TextDataset(test_df.제목_키워드.tolist(), None, tokenizer)  # 라벨 없음\n\n# # 데이터 로더 생성\n# train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True)\n# val_loader = DataLoader(val_dataset, batch_size=CFG.batch_size, shuffle=False)\n# test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:20:41.455501Z","iopub.execute_input":"2024-10-14T06:20:41.455809Z","iopub.status.idle":"2024-10-14T06:20:41.469041Z","shell.execute_reply.started":"2024-10-14T06:20:41.455769Z","shell.execute_reply":"2024-10-14T06:20:41.468280Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n# #tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\n# model = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=len(train_df['분류'].unique())).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:20:41.470012Z","iopub.execute_input":"2024-10-14T06:20:41.470322Z","iopub.status.idle":"2024-10-14T06:20:41.481314Z","shell.execute_reply.started":"2024-10-14T06:20:41.470292Z","shell.execute_reply":"2024-10-14T06:20:41.480461Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# 데이터 준비\nfinal_df['제목_키워드'] = final_df['제목'] + ' ' + final_df['키워드']\ntest_df['제목_키워드'] = test_df['제목'] + ' ' + test_df['키워드']\n\n# 레이블 인코딩\nlabel_encoder = {label: i for i, label in enumerate(final_df['분류'].unique())}\nfinal_df['label'] = final_df['분류'].map(label_encoder)\n\n# 데이터 분할 (final_df -> train + validation)\ntrain_df, val_df = train_test_split(final_df, test_size=0.2, stratify=final_df['분류'], random_state=42)\n\n# 데이터셋 생성\ntrain_dataset = TextDataset(train_df.제목_키워드.tolist(), train_df.label.tolist(), tokenizer)\nval_dataset = TextDataset(val_df.제목_키워드.tolist(), val_df.label.tolist(), tokenizer)\ntest_dataset = TextDataset(test_df.제목_키워드.tolist(), None, tokenizer)  # 라벨 없음\n\n# 데이터 로더 생성\ntrain_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=CFG.batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:20:41.482461Z","iopub.execute_input":"2024-10-14T06:20:41.482848Z","iopub.status.idle":"2024-10-14T06:20:41.633705Z","shell.execute_reply.started":"2024-10-14T06:20:41.482806Z","shell.execute_reply":"2024-10-14T06:20:41.632864Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# 옵티마이저 및 학습 파라미터 설정\n#optimizer = AdamW(model.parameters(), lr=CFG.learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:20:41.636236Z","iopub.execute_input":"2024-10-14T06:20:41.636561Z","iopub.status.idle":"2024-10-14T06:20:41.640717Z","shell.execute_reply.started":"2024-10-14T06:20:41.636527Z","shell.execute_reply":"2024-10-14T06:20:41.639804Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# # 학습\n# model.train()\n# #for epoch in range(CFG.epoch):\n# for epoch in range(CFG.epoch):\n#     for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{CFG.epoch}'):\n#         optimizer.zero_grad()\n#         input_ids = batch['input_ids'].to(device)\n#         attention_mask = batch['attention_mask'].to(device)\n#         labels = batch['labels'].to(device)\n#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n#         loss = outputs.loss\n#         loss.backward()\n#         optimizer.step()\n\n#     # Validation\n#     model.eval()\n#     val_predictions = []\n#     val_true_labels = []\n#     with torch.no_grad():\n#         for batch in tqdm(val_loader, desc='Validating'):\n#             input_ids = batch['input_ids'].to(device)\n#             attention_mask = batch['attention_mask'].to(device)\n#             labels = batch['labels'].to(device)\n#             outputs = model(input_ids, attention_mask=attention_mask)\n#             _, preds = torch.max(outputs.logits, dim=1)\n#             val_predictions.extend(preds.cpu().tolist())\n#             val_true_labels.extend(labels.cpu().tolist())\n    \n#     # 검증 결과 출력\n#     val_f1 = f1_score(val_true_labels, val_predictions, average='macro')\n#     print(f\"Validation F1 Score: {val_f1:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:20:41.642123Z","iopub.execute_input":"2024-10-14T06:20:41.642439Z","iopub.status.idle":"2024-10-14T06:20:41.650324Z","shell.execute_reply.started":"2024-10-14T06:20:41.642407Z","shell.execute_reply":"2024-10-14T06:20:41.649581Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"config = {\n    \"learning_rate\": 0.0001,   # 학습률\n    \"epoch\": 15,               # 에폭 수\n    \"batch_size\": 64,          # 배치 크기\n    \"momentum\": 0.9,           # 모멘텀\n    \"weight_decay\": 0.0005,    # 가중치 감쇠\n    \"dropout_rate\": 0.3,       # 드롭아웃 비율\n    \"gradient_clipping\": 1.0   # 그래디언트 클리핑\n}\n\nCFG = SimpleNamespace(**config)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:20:41.651220Z","iopub.execute_input":"2024-10-14T06:20:41.651494Z","iopub.status.idle":"2024-10-14T06:20:41.662412Z","shell.execute_reply.started":"2024-10-14T06:20:41.651464Z","shell.execute_reply":"2024-10-14T06:20:41.661583Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 토크나이저 초기화 (BertTokenizer 사용)\ntokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n\n# BERT 구성 설정에서 드롭아웃 비율 변경 (예: 0.3으로 설정)\nconfig = BertConfig.from_pretrained('monologg/kobert', hidden_dropout_prob=0.3, num_labels=len(train_df['분류'].unique()))\n\n# 모델 초기화 (드롭아웃 비율 적용)\nmodel = BertForSequenceClassification.from_pretrained(\n    'monologg/kobert', \n    config=config\n).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:23:15.767961Z","iopub.execute_input":"2024-10-14T06:23:15.768369Z","iopub.status.idle":"2024-10-14T06:23:16.179426Z","shell.execute_reply.started":"2024-10-14T06:23:15.768330Z","shell.execute_reply":"2024-10-14T06:23:16.178389Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \nThe class this function is called from is 'BertTokenizer'.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score\n\n# 모델 학습 함수\ndef train_and_validate(model, train_loader, val_loader, optimizer, scheduler, CFG, device):\n    model.train()\n    best_val_f1 = 0\n    patience = 3\n    early_stopping_counter = 0\n\n    for epoch in range(CFG.epoch):\n        # Training\n        model.train()\n        total_loss = 0\n        for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{CFG.epoch}'):\n            optimizer.zero_grad()\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            # Forward pass\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            loss.backward()\n            \n            # Gradient Clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.gradient_clipping)\n            \n            optimizer.step()\n            total_loss += loss.item()\n        \n        avg_train_loss = total_loss / len(train_loader)\n        print(f\"Epoch {epoch + 1}/{CFG.epoch}, Training Loss: {avg_train_loss:.4f}\")\n        \n        # Validation\n        model.eval()\n        val_predictions = []\n        val_true_labels = []\n        val_loss = 0\n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc='Validating'):\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                \n                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n                val_loss += outputs.loss.item()\n                _, preds = torch.max(outputs.logits, dim=1)\n                val_predictions.extend(preds.cpu().tolist())\n                val_true_labels.extend(labels.cpu().tolist())\n        \n        avg_val_loss = val_loss / len(val_loader)\n        val_f1 = f1_score(val_true_labels, val_predictions, average='macro')\n        \n        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation F1 Score: {val_f1:.2f}\")\n        \n        # Early Stopping\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            early_stopping_counter = 0\n        else:\n            early_stopping_counter += 1\n            if early_stopping_counter >= patience:\n                print(\"Early stopping triggered!\")\n                break\n        \n        # Learning Rate Scheduler step\n        scheduler.step()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:23:20.829794Z","iopub.execute_input":"2024-10-14T06:23:20.830518Z","iopub.status.idle":"2024-10-14T06:23:20.844449Z","shell.execute_reply.started":"2024-10-14T06:23:20.830474Z","shell.execute_reply":"2024-10-14T06:23:20.843249Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=CFG.learning_rate, weight_decay=CFG.weight_decay)\nscheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # 매 5 에폭마다 학습률을 10% 감소\n\n# Train and validate (train_loader와 val_loader가 이미 정의되어 있다고 가정)\ntrain_and_validate(model, train_loader, val_loader, optimizer, scheduler, CFG, device)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:23:21.551675Z","iopub.execute_input":"2024-10-14T06:23:21.552575Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1/15:   4%|▍         | 19/425 [00:26<09:02,  1.34s/it]","output_type":"stream"}]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"model.eval()\ntest_predictions = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc='Testing'):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        \n        # 모델을 통한 예측\n        outputs = model(input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs.logits, dim=1)\n        \n        # 예측 결과 저장\n        test_predictions.extend(preds.cpu().tolist())\n\n# test.csv 파일에서 'id' 값을 그대로 사용\ntest_df = pd.read_csv('/kaggle/input/dacon-dataset/test.csv')  # test.csv에 'id' 칼럼이 존재한다고 가정\n\n# 라벨 디코딩\nlabel_decoder = {i: label for label, i in label_encoder.items()}\ndecoded_predictions = [label_decoder[pred] for pred in test_predictions]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:20:41.855224Z","iopub.status.idle":"2024-10-14T06:20:41.855561Z","shell.execute_reply.started":"2024-10-14T06:20:41.855390Z","shell.execute_reply":"2024-10-14T06:20:41.855408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 예측 결과를 데이터프레임으로 저장\ndf_results = pd.DataFrame({\n    'ID': test_df['ID'],             # test.csv의 'id' 값을 그대로 사용\n    '분류': decoded_predictions  # 디코딩된 예측 라벨\n})\n\n# CSV 파일로 저장\ndf_results.to_csv('/kaggle/working/submission.csv', index=False)\n\nprint(\"예측 결과가 submission.csv 파일로 저장되었습니다.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:20:41.856617Z","iopub.status.idle":"2024-10-14T06:20:41.856989Z","shell.execute_reply.started":"2024-10-14T06:20:41.856808Z","shell.execute_reply":"2024-10-14T06:20:41.856828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/working/submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:20:41.858606Z","iopub.status.idle":"2024-10-14T06:20:41.858983Z","shell.execute_reply.started":"2024-10-14T06:20:41.858801Z","shell.execute_reply":"2024-10-14T06:20:41.858820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(50)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:20:41.860094Z","iopub.status.idle":"2024-10-14T06:20:41.860428Z","shell.execute_reply.started":"2024-10-14T06:20:41.860259Z","shell.execute_reply":"2024-10-14T06:20:41.860276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}